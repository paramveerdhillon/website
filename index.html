<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Paramveer Dhillon</title>
  <meta name="description" content="Research in machine learning, artificial intelligence,social networks, network science, natural language processing, computational social science and related areas" />
  <meta name="keywords" content="machine learning, artificial intelligence,social networks, network science, natural language processing, computational social science" />

  <style>
    /* --- mobile consistency --- */
    html { -webkit-text-size-adjust: 100%; text-size-adjust: 100%; }
    ol li::marker { font-size: 1em; }

    /* --- base typography / layout (keeps original look) --- */
    body, p, h1, h2, h3 {
      font-family: monaco, Consolas, "Lucida Console", monospace;
    }
    body, p { font-size: 14px; }
    h3 { font-size: 12px; }

    body {
      padding-left: 5px;
      padding-right: 5px;
      text-align: left;
      background: #000;
      color: #fff;
    }

    p, h1, h2 {
      padding-left: 5px;
      padding-right: 5px;
    }

    /* --- link colors (matches body link/vlink/alink) --- */
    a, a:visited { color: #00FF00; }
    a:active { color: red; }

    /* --- color utility classes (replaces <font color=...>) --- */
    :root { --magenta:#FF00FF; --crimson:#C70039; --cyan:#00FFFF; --orange:#FFA500; }
    .c-white { color: #fff; }
    .c-black { color: #000; }
    .c-magenta { color: var(--magenta); }
    .c-crimson { color: var(--crimson); }
    .c-cyan { color: var(--cyan); }
    .c-orange { color: var(--orange); }
    .c-yellow { color: yellow; }
    .c-red { color: red; }
    .c-green { color: green; }
    .c-darkred { color: darkred; }
    .c-blue { color: blue; }

    /* anchors that were intentionally white text inside links */
    .link-white, .link-white:visited { color: #fff; }
    .link-white:active { color: red; }

    /* --- updates list (same as before, just not duplicated inline) --- */
    ul.updates { list-style-type: disc; padding-left: 25px; margin-top: 6px; }
    ul.updates li { margin: 4px 0; }
    ul.updates time { font-style: italic; }

    /* --- students section (kept as-is, just grouped) --- */
    .students h2 { color: var(--magenta); text-decoration: underline; margin: .25rem 0 .5rem; }
    .students h3 { color: var(--crimson); text-decoration: underline; margin: 1rem 0 .5rem; }

    /* multi-column ordered lists (keeps numbering) */
    .ol-cols { column-gap: 2rem; column-fill: balance; }
    @media (min-width:700px){ .ol-cols { column-count: 2; } }
    @media (min-width:1100px){ .ol-cols { column-count: 3; } }
    .ol-cols li { break-inside: avoid; -webkit-column-break-inside: avoid; margin-bottom: .35rem; }

    /* per-item colors (as before) */
    .tag { color: var(--cyan); font-weight: 600; }                 /* [Fxx–] style */
    .meta-phd { color: var(--orange); font-style: italic; }        /* PhD "Last Stop" */
    .meta-former { color: var(--cyan); }                           /* Former-student details */
  </style>
</head>

<body>
  <hr>

  <table>
    <tr>
      <td>
        <img src="photos/great-wall.png" width="250" style="border:2px solid white;" hspace="5" alt="[PHOTO]" />

        <a name="contact" id="contact"></a>
        <p></p>
        <p></p>
        <p></p>

        <h1><span class="c-magenta">PARAMVEER DHILLON</span></h1>

        <p>
          Associate Professor<br>
          <a href="https://www.si.umich.edu">School of Information</a><br>
          <!--<a href="https://cse.engin.umich.edu">Computer Science &amp; Engineering</a> <span class="c-cyan">(courtesy)</span><br> -->
          <a href="https://www.umich.edu"> University of Michigan</a><br><br>

          Affiliate Faculty<br>
          <a href="https://midas.umich.edu">Michigan Institute for Data Science (MIDAS)</a><br>
          <a href="https://e-hail.umich.edu">E-Health &amp; AI Initiative (e-HAIL)</a><br><br><br>

          Digital Fellow<br>
          <a href="http://ide.mit.edu"> MIT Initiative on the Digital Economy</a>
          <br>
        </p>

        <p></p>
        <p></p>

        <p>
          <span class="c-cyan">Office:</span>5544 Leinweber, 2200 Hayward St., Ann Arbor, MI 48109<br>
          <span class="c-cyan">Phone:</span> 734-764-5876<br>
          <span class="c-cyan">Email:</span> lastname followed by the letter 'p' at umich dot edu<br>
          <span class="c-cyan">Twitter:</span> <a href="https://twitter.com/dhillon_p">@dhillon_p</a><br>
          <span class="c-cyan">Google Scholar:</span> <a href="https://scholar.google.co.in/citations?user=vu5Mw_0AAAAJ&hl=en">https://goo.gl/FEsnE8</a><br>
        </p>
      </td>
    </tr>
  </table>

  <hr>
  <strong><span class="c-magenta"><u>Quick Navigation Links</u></span></strong>
  <p>Please follow the links below to navigate to specific subsections of the site or just scroll down to view all the content.</p>

  <p>
    <a href="#researchinterests">Research Interests</a>
    &nbsp;
    <a href="#updates">Recent Updates</a>
    &nbsp;
    <a href="#selectpubs">Publications</a>
    &nbsp;
    <a href="#profbio">Professional Background</a>
    &nbsp;
    <a href="#teaching">Teaching</a>
    &nbsp;
    <a href="#awards">Awards</a>
    &nbsp;
    <a href="#students">Research Group</a>
    &nbsp;
    <a href="#service">Service</a>
    &nbsp;
    <a href="#software">Software</a>
  </p>

  <hr>

  <a name="researchinterests" id="researchinterests"></a>
  <strong><span class="c-magenta"><u>Research Interests</u></span></strong>

  <p>My current research focuses on how Large Language Models (LLMs) reshape human creativity, decision-making, and information consumption. My work bridges the disciplines of AI, Human-Computer Interaction (HCI), and Information Systems and is broadly situated in Human-Centered AI.</p>

  <p><strong class="c-yellow">Key Research Areas:</strong></p>

  <p><strong><span class="c-orange">Human-LLM Collaboration/Co-writing and Creative Labor Markets</span></strong>: We investigate how people create with LLMs, examining questions of authorship, ownership, and market disruption. Our empirical work provides the first systematic evidence that fine-tuned LLMs can produce undetectable, professional-quality writing that competes directly with human authors.</p>

  <p><strong><span class="c-orange">Personalization and Human-Centric Recommender Systems</span></strong>: We design LLM-based systems that respect human agency while providing personalized experiences. This includes developing temptation-aware recommendation algorithms that help users navigate between immediate desires and long-term goals, and creating personalization methods that adapt to individual preferences without reinforcing filter bubbles.</p>

  <p><strong><span class="c-orange">Causal Methods for Language-Based Interventions</span></strong>: We advance causal inference techniques for high-dimensional text treatments. Our recent work introduces policy learning frameworks for natural language action spaces, enabling LLMs to learn optimal intervention strategies through gradient-based optimization on language embeddings. This line of work supports a variety of applications ranging from therapeutic dialogue refinement to content moderation, where each text-based decision impacts future outcomes.</p>

  <p><strong class="c-yellow">Active Research Threads:</strong></p>
  <ul>
    <li><span class="c-cyan">Empirical measurement of LLM impacts on creative labor markets and copyright</span> [ArXiv '25b <a href="https://arxiv.org/abs/2510.13939">arXiv:2510.13939</a>].</li>
    <li><span class="c-orange">Temptation-aware recommender systems that balance engagement with user well-being</span> [RecSys '25, WWW '24, Management Science '24, Marketing Science '21]</li>
    <li><span class="c-cyan">Multi-stage decision-making with natural language actions</span>  [ArXiv '25a; <a href="https://arxiv.org/abs/2502.17538">arXiv:2502.17538</a>].</li>
    <li><span class="c-orange">Psychological ownership and agency in human-LLM collaboration</span>.</li>
    <li><span class="c-cyan">Causal frameworks for evaluating text-based interventions</span> [NAACL '24].</li>
  </ul>

  <hr>
  <a name="updates" id="updates"></a>
  <strong><span class="c-magenta"><u>Recent Updates</u></span></strong>

  <ul class="updates">
    <li><time datetime="2026-01-10">Jan 2026</time>— <strong><span class="c-green">Paper</span></strong>: CHI '26 paper on "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High Quality Books" <a href="">arXiv</a>.</li>

     <li><time datetime="2026-01-10">Jan 2026</time>— <strong><span class="c-green">Paper</span></strong>: New paper on "Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing" <a href="http://arxiv.org/abs/2601.10236">arXiv</a>.</li>
    
    <li><time datetime="2026-01-10">Jan 2026</time>— <strong><span class="c-orange">Appointment</span></strong>: Excited to be an Associate Editor (AE) at <a href="https://pubsonline.informs.org/page/mnsc/editorial-board">Management Science</a>.</li>

    <li><time datetime="2026-01-03">Jan 2026</time> — <strong><span class="c-orange">Paper</span></strong>: EACL '26 paper on "Ex-Ante Inference in LLMs" <a href="https://arxiv.org/pdf/2505.19533">arXiv</a>.</li>

    <li><time datetime="2025-10-25">Oct 2025</time> — <strong><span class="c-cyan">Paper</span></strong>: New paper on AI and Copyright <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5606570">SSRN</a>. [Coverage in the <a href="https://www.newyorker.com/culture/the-weekend-essay/what-if-readers-like-ai-generated-fiction">New Yorker</a>, <a href="https://marginalrevolution.com/marginalrevolution/2025/11/do-readers-prefer-ai-writers.html">Marginal Revolution</a> (by Tyler Cowen).]</li>

    <li><time datetime="2025-10-27">Oct 2025</time> — <strong><span class="c-green">Award</span></strong>: Best Paper Award to our paper on "Widespread LLM Usage and Evaluator Behavior in Credit Screening" @ INFORMS e-Business Society <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5616650">SSRN</a>.</li>

    <li><time datetime="2025-10-25">Oct 2025</time> — <strong><span class="c-cyan">Travel</span></strong>: Travel to CIST/INFORMS @ Atlanta, GA.</li>

    <li><time datetime="2025-10-15">Oct 2025</time> — <strong><span class="c-orange">Paper</span></strong>: CSCW '25 paper on "Unexpected engagement patterns on Twitter" <a href="https://arxiv.org/pdf/2509.08128">arXiv</a>.</li>

    <li><time datetime="2025-09-25">Sep 2025</time> — <strong><span class="c-orange">Paper</span></strong>: RecSys '25 paper on "Recommendation and Temptation" <a href="https://arxiv.org/abs/2412.10595">arXiv</a>.</li>

    <li><time datetime="2025-06-20">Jun 2025</time> — <strong><span class="c-green">Award Finalist</span></strong>: "Targeting for Long-term outcomes" paper was a finalist for John D. C. Little Best Paper Award!</li>

    <li><time datetime="2025-06-15">Jun 2025</time> — <strong><span class="c-orange">Paper</span></strong>: ACL '25 (Findings) paper on "Dual Debiasing for Noisy In-context Learning (ICL)" <a href="https://arxiv.org/pdf/2506.00418">arXiv</a>.</li>

    <li><time datetime="2025-05-15">May–Aug 2025</time> — <strong><span class="c-cyan">Travel</span></strong>: Travel to Shanghai and Tokyo for summer teaching. Say hi if you're around!</li>

    <li><time datetime="2025-02-24">Feb 2025</time> — <strong><span class="c-orange">Paper</span></strong>: New preprint on "Policy Learning in Combinatorial Natural Language Action Spaces" <a href="https://arxiv.org/pdf/2502.17538">arXiv</a>.</li>

    <li><time datetime="2025-02-01">Feb 2025</time> — <strong><span class="c-orange">Paper</span></strong>: PNAS Nexus paper on "How Digital Paywalls Shape News Coverage" <a href="https://academic.oup.com/pnasnexus/article/4/1/pgae511/7960035">PDF</a>.</li>

  </ul>

  <p></p>
  <hr>

  <a name="profbio" id="profbio"></a>
  <strong><span class="c-magenta"><u>Professional Background</u></span></strong>

  <p>
    I am an Associate Professor in the <a href="https://www.si.umich.edu">School of Information</a> at the <a href="https://www.umich.edu">University of Michigan</a> (tenured 2025) and a Digital Fellow at <a href="http://ide.mit.edu">MIT's Initiative on the Digital Economy</a>. I joined Michigan as an Assistant Professor in 2019.
  </p>

  <p>
    I hold an A.M. in <a href="https://statistics.wharton.upenn.edu">Statistics</a> and an M.S.E. and a Ph.D. (2015) in <a href="https://www.cis.upenn.edu">Computer Science</a> all from the <a href="https://www.upenn.edu">University of Pennsylvania</a>, where I was advised by Professors <a href="http://www.cis.upenn.edu/~ungar/">Lyle Ungar</a>, <a href="http://deanfoster.net/index.pl">Dean Foster</a>, and <a href="https://www.med.upenn.edu/apps/faculty/index.php/g5455356/p10656">James Gee</a>. My dissertation, <span class="c-cyan">"Advances in Spectral Learning with Applications to Text Analysis and Brain Imaging,"</span> received the <span class="c-orange">Morris and Dorothy Rubinoff Award</span> for outstanding doctoral dissertation. This work introduced theoretically-grounded spectral methods for learning word embeddings (<span class="c-cyan">JMLR 2015</span>, <span class="c-cyan">ICML 2012</span>, <span class="c-cyan">NeurIPS 2011</span>) and brain image segmentation (<span class="c-cyan">NeuroImage 2014</span>), achieving both computational efficiency and provable convergence guarantees. My contributions to spectral decomposition and context-dependent representation learning provided early theoretical foundations for understanding how distributed representations capture semantic relationships, principles that remain central to modern transformer architectures. I also did other research in my Ph.D. on establishing connections between PCA and ridge regression (<span class="c-cyan">JMLR 2013</span>) and on provably faster row and column subsampling algorithms for least squares regression (<span class="c-cyan">NeurIPS 2013a,b</span>).
  </p>

  <p>
    Following my Ph.D., I completed a postdoctoral fellowship at <a href="http://www.web.mit.edu/">MIT</a> with Professor <a href="https://mitsloan.mit.edu/faculty/directory/sinan-kayhan-aral">Sinan Aral</a>, where I worked on problems at the intersection of Machine Learning, Causal Inference, Network Science, and Information Systems. This research program produced several foundational contributions: establishing tractable methods for influence maximization under empirically-grounded network models (<span class="c-cyan">Nature Human Behaviour 2018</span>); designing optimal digital paywall strategies that balance subscription revenue with content demand leveraging quasi-experiments (<span class="c-cyan">Management Science 2020</span>); developing neural matrix factorization techniques for modeling temporal dynamics in user preferences (<span class="c-cyan">Marketing Science 2021</span>); quantifying the information advantages of network brokers through novel diversity metrics (<span class="c-cyan">Management Science 2023</span>); and creating surrogate-index methods for optimizing long-term outcomes in sequential decision problems (<span class="c-cyan">Management Science 2024</span>).
  </p>

  <p>
    Much before all this, I was a carefree undergrad studying Electronics &amp; Electrical Communication Engineering at <a href="https://pec.ac.in">PEC</a> in my hometown of <a href="https://en.wikipedia.org/wiki/Chandigarh">Chandigarh</a>, India. I developed my interest in AI/ML and the desire to pursue a Ph.D. as a result of three memorable summer internships, before my Ph.D., at <a href="http://www.cvc.uab.es/?page_id=226">Computer Vision Center @ Barcelona</a> <span class="c-cyan">[summer 2006]</span>, <a href="https://ei.is.tuebingen.mpg.de">Max Planck Institute for Intelligent Systems @ Tuebingen</a> <span class="c-cyan">[summer 2008]</span>, and <a href="https://www.isi.edu/research_groups/nlg/home">Information Sciences Institute/USC @ Los Angeles</a> <span class="c-cyan">[summer 2009]</span>.
  </p>

  <a name="teaching" id="teaching"></a>
  <hr>
  <strong><span class="c-magenta"><u> Teaching</u></span></strong>

  <p>
    <ol>
      <li><span class="c-cyan">ECE 4760J</span><em><span class="c-orange"> Data Mining: Methods and Applications</span></em> <span class="c-darkred">@ UM-Shanghai Jiao Tong University Joint Institute (JI)</span>  @  S25.<br></li>
      <li><span class="c-cyan">SI 671/721</span><em><span class="c-orange"> Data Mining: Methods and Applications</span></em> @  F[19-24].<br></li>
      <li><span class="c-cyan">SIADS 642 [online]</span><em><span class="c-orange"> Deep Learning I</span></em> <span class="c-darkred">[Developed from scratch]</span> @  F20-present.<br></li>
      <li><span class="c-cyan">SIADS 647 [online]</span><em><span class="c-orange"> Deep Learning II (Generative AI)</span></em> <span class="c-darkred">[Developed from scratch]</span>  @  W25.<br></li>
      <li><span class="c-cyan">SIADS 532,632 [online]</span><em><span class="c-orange"> Data Mining I, II</span></em>  @  W21-present.<br></li>
    </ol>
  </p>

  <a id="students"></a>
  <hr class="section">

  <section class="students">
    <h2>Research Group</h2>

    <h3>Ph.D. Students</h3>
    <ol class="ol-cols">
      <li>Yachuan Liu <span class="tag">[F20–]</span> <span class="meta-phd">— Last Stop: BS @ UC Berkeley</span></li>
      <li>Sanzeed Anwar <span class="tag">[F21–]</span> <span class="meta-phd">— Last Stop: BS+MEng @ MIT</span></li>
      <li>Bohan Zhang <span class="tag">[F22–]</span> <span class="meta-phd">— Last Stop: MS @ University of Michigan</span></li>
    </ol>

    <h3>Former Students</h3>
    <ol class="ol-cols">
      <li>Xiaochun Wei <span class="meta-former">[BS ’25, next M.S. in Data Science at Harvard University]</span></li>
      <li>Jiyu Chen <span class="meta-former">[BS ’23, MS ’24, next Software Engineer at Stripe]</span></li>
      <li>Xinyue Li <span class="meta-former">[MS ’23, next Ph.D. in Statistics at Boston University]</span></li>
      <li>Ella Li <span class="meta-former">[MS ’23, next Ph.D. in CS at Northeastern University]</span></li>
      <li>Siqi Ma <span class="meta-former">[BS ’23, next MS in Statistics at Stanford University]</span></li>
      <li>Shaochun Zheng <span class="meta-former">[BS ’23, next MS in CS at UC San Diego]</span></li>
      <li>Houming Chen <span class="meta-former">[BS ’23, next Ph.D. in CS at University of Michigan]</span></li>
      <li>Yushi She <span class="meta-former">[BS ’23, next MS in CS at Georgia Tech]</span></li>
      <li>Ted Yuan <span class="meta-former">[BS ’23, next MS in ECE at Carnegie Mellon University]</span></li>
      <li>Evan Weissburg <span class="meta-former">[BS ’23, next Software Engineer at Jane Street Capital]</span></li>
      <li>Arya Kumar <span class="meta-former">[BS ’23, next Software Engineer at Jane Street Capital]</span></li>
      <li>Jupiter Zhu <span class="meta-former">[BS ’22, next MS in CS at Stanford University]</span></li>
      <li>Tianyi Li <span class="meta-former">[BS ’22, next MS in INI at Carnegie Mellon University]</span></li>
      <li>Xianglong Li <span class="meta-former">[BS ’22, next MS in CS at Yale University]</span></li>
      <li>Florence Wu <span class="meta-former">[BS ’22, next MS in CS at Harvard University]</span></li>
      <li>Yingzhuo Yu <span class="meta-former">[BS ’22, next MS in CS at UIUC]</span></li>
      <li>Xingjian Zhang <span class="meta-former">[BS ’22, next Ph.D. in Information at UMSI]</span></li>
      <li>Bohan Zhang <span class="meta-former">[MS ’22, next Ph.D. in Information at UMSI]</span></li>
      <li>Zhengyang Shan <span class="meta-former">[MS ’22, next Ph.D. in CDS at Boston University]</span></li>
      <li>Jiapeng Guo <span class="meta-former">[BS ’21, next MS in CS at Columbia University]</span></li>
      <li>Zilu Wang <span class="meta-former">[BS ’21, next MS in MS&amp;E at Stanford University]</span></li>
    </ol>

    <p class="note">
      I supervise students interested in <span class="label">Human-centric AI and Information Systems</span>. For Fall ’26, I’m specifically recruiting Ph.D. students with strong AI/HCI skills—e.g., building polished front-ends (React/TypeScript), rapid prototyping and performing online user studies, fine-tuning and evaluating LLMs, and solid data/ML engineering/programming skills. The nature of the research project will be similar in spirit to <a href="https://arxiv.org/abs/2510.13939">this paper</a>.
    </p>

    <p class="note">
      <em>Masters/Undergrads (already at University of Michigan)</em> may email their CV and transcripts. <em>Prospective Ph.D. students</em> are encouraged to apply to our Ph.D. program
      <a href="https://www.si.umich.edu/programs/phd-information/how-do-i-apply">here</a> and mention my name as a potential advisor. The deadline is December 1 each year.
    </p>
  </section>

  <a name="awards" id="awards"></a>
  <hr>
  <strong><span class="c-magenta"><u>Awards</u></span></strong>

  <p>
    <ol>
      <li>INFORMS Annual Conference <span class="c-cyan"> (e-Business Society; Best Paper Award),</span> <span class="c-crimson">2025.</span></li>
      <li>John D. C. Little Best Paper Award <span class="c-cyan"> (Finalist),</span> <span class="c-crimson">2025.</span></li>
      <li>INFORMS Information Systems Society (ISS) <span class="c-cyan"> Gordon B. Davis Young Scholar Award,</span> <span class="c-crimson">2021.</span></li>
      <li>INFORMS Annual Conference <span class="c-cyan"> (e-Business Society; Best Paper Award),</span> <span class="c-crimson">2020.</span></li>
      <li>Workshop on Information Systems and Economics (WISE) <span class="c-cyan"> (Runner-up Best Paper Award),</span> <span class="c-crimson">2016.</span></li>
      <li>Rubinoff Best Doctoral Dissertation Award <span class="c-cyan">(awarded by <a href="https://www.cis.upenn.edu">Penn CIS</a>),</span> <span class="c-crimson">2015.</span></li>
    </ol>
  </p>

  <a name="service" id="service"></a>
  <hr>
  <strong><span class="c-magenta"><u>Service to the Profession</u></span></strong>

  <p>
    <ol>
      <li><a class="link-white" href="https://pubsonline.informs.org/page/mnsc/editorial-board">Associate Editor</a> <span class="c-crimson">Management Science [2026-].</span></li>
      <li><a class="link-white" href="https://jmlr.org/editorial-board.html">Editorial Board</a> <span class="c-crimson">JMLR [2020-].</span></li>
      <li>Ad-hoc Reviewer: <span class="c-crimson">Nature</span>, <span class="c-crimson">Nature Human Behaviour</span>, <span class="c-crimson">Nature Communications</span>, <span class="c-crimson">PNAS</span>, <span class="c-crimson">JAIR</span>, <span class="c-crimson">Information Science Research (ISR)</span>, <span class="c-crimson">Management Science</span>, <span class="c-crimson">Marketing Science</span>, <span class="c-crimson">IEEE TKDE</span>, <span class="c-crimson">IEEE TPAMI.</span></li>
      <li>Reviewer/PC/SPC Member @ Core AI/ML Conferences: <span class="c-cyan">[every year since 2013]</span> <span class="c-crimson">NeurIPS</span>, <span class="c-crimson">ICML</span>, <span class="c-crimson">AISTATS</span>, <span class="c-crimson">ICLR</span>, <span class="c-crimson">AAAI</span>, <span class="c-crimson">IJCAI.</span></li>
      <li>Reviewer/PC/SPC Member @ Core Information Systems Conferences: <span class="c-cyan">[every year since 2017]</span> <span class="c-crimson">ICIS</span>, <span class="c-crimson">CIST,</span> <span class="c-crimson">WISE.</span></li>
      <li>Reviewer/PC/SPC Member @ Core NLP/Computational Social Science Conferences: <span class="c-cyan">[sporadically]</span> <span class="c-crimson">The Web Conference (WWW)</span>, <span class="c-crimson">EMNLP</span>, <span class="c-crimson">NAACL</span>, <span class="c-crimson">ICWSM</span>, <span class="c-crimson">IC2S2.</span></li>
    </ol>
  </p>

  <a name="selectpubs" id="selectpubs"></a>
  <hr>

  <strong><span class="c-magenta"><u> Selected Publications</u></span></strong>

  <p>
    <span class="c-red"> Below is a list of selected publications that highlight my core research interests and contributions.</span>
    A complete list of all my publications is available <a href="https://goo.gl/FEsnE8">here</a>.
  </p>

  <p><sup>*</sup>indicates alphabetical author listing.</p>

  <ul>
    <li>
      <p>
        <em><span class="c-orange">Recommendation and Temptation.</span></em><br>
        Sanzeed Anwar, <span class="c-cyan">Paramveer Dhillon</span>, and Grant Schoenebeck.<br>
        <span class="c-crimson">RecSys</span> <span class="c-yellow">(ACM Conference on Recommender Systems)</span>, 2025.<br>
        <a href="https://arxiv.org/abs/2412.10595">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">How Digital Paywalls Shape News Coverage.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Anmol Panda, and Libby Hemphill.<br>
        <span class="c-crimson">PNAS Nexus</span>, 2025.<br>
        <a href="https://academic.oup.com/pnasnexus/article/4/1/pgae511/7960035">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Causal Inference for Human-Language Model Collaboration.</span></em><br>
        Bohan Zhang, Yixin Wang, and <span class="c-cyan">Paramveer Dhillon.</span><br>
        <span class="c-crimson">NAACL(Main Conference)</span> <span class="c-yellow">(Annual Conference of the North American Chapter of ACL)</span>, 2024.<br>
        <a href="https://arxiv.org/abs/2404.00207">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Somayeh Molaei, Jiaqi Li, Maximilian Golub, Shaochun Zheng, and Lionel Robert.<br>
        <span class="c-crimson">CHI</span> <span class="c-yellow">(SIGCHI Conference on Human Factors in Computing Systems)</span>, 2024.<br>
        <a href="https://arxiv.org/abs/2402.11723">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns.</span></em><br>
        Sanzeed Anwar, Grant Schoenebeck, and <span class="c-cyan">Paramveer Dhillon.</span><br>
        <span class="c-crimson">WWW</span> <span class="c-yellow">(The Web Conference)</span>, 2024.<br>
        <a href="https://arxiv.org/abs/2402.15013">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Targeting for long-term outcomes.</span></em><br>
        Jeremy Yang, Dean Eckles, <span class="c-cyan">Paramveer Dhillon</span>, and Sinan Aral.<br>
        <span class="c-crimson">Management Science</span>, 2023.<br>
        <a href="papers/targeting-long-term.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">What (Exactly) is Novelty in Networks? Unpacking the Vision Advantages of Brokers, Bridges, and Weak Ties.</span></em><br>
        Sinan Aral, <span class="c-cyan">Paramveer Dhillon</span>.<br>
        <span class="c-crimson">Management Science</span>, 2022.<br>
        <a href="papers/aral-dhillon-novelty.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Modeling Dynamic User Interests: A Neural Matrix Factorization Approach.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Sinan Aral.<br>
        <span class="c-crimson">Marketing Science</span>, 2021.<br>
        <a href="papers/dhillon-aral-dynamic-interests-mksc.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Digital Paywall Design: Implications for Content Demand &amp; Subscriptions.<sup>*</sup></span></em><br>
        Sinan Aral, <span class="c-cyan">Paramveer Dhillon.</span><br>
        <span class="c-crimson">Management Science</span>, 2020.<br>
        <a href="papers/dhillon20Paywall.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Social Influence Maximization under Empirical Influence Models.<sup>*</sup></span></em><br>
        Sinan Aral, <span class="c-cyan">Paramveer Dhillon.</span><br>
        <span class="c-crimson">Nature Human Behaviour</span>, 2018.<br>
        <a href="https://www.nature.com/articles/s41562-018-0346-z.epdf?author_access_token=N3H-02ldeE4Au_GNS-_Sl9RgN0jAjWel9jnR3ZoTv0PVP3dTdNThCuDLo3WU2Y7zDTMMUjiXDkdH1FtGlpFLC9mswZaf5ycdN2RqIvrlWelLux3HrjtdxwCVD7Cjkt_1NCfXSrl3l-i5NieYjFWGqg%3D%3D">[PDF]</a>
        <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-018-0346-z/MediaObjects/41562_2018_346_MOESM1_ESM.pdf">[Supplementary Information]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Eigenwords: Spectral Word Embeddings.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">JMLR</span> <span class="c-yellow">(Journal of Machine Learning Research)</span>, 2015.<br>
        <a href="papers/dhillon15a.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">New Subsampling Algorithms for Fast Least Squares Regression.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Yichao Lu, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">NeurIPS</span> <span class="c-yellow">(Advances in Neural Information Processing Systems Conference)</span>, 2013.<br>
        <a href="papers/uluruNIPS2013.pdf">[PDF]</a>
        <a href="papers/uluru_appendix.pdf">[Supplementary Information]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Faster Ridge Regression via the Subsampled Randomized Hadamard Transform.</span></em><br>
        Yichao Lu, <span class="c-cyan">Paramveer Dhillon</span>, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">NeurIPS</span> <span class="c-yellow">(Advances in Neural Information Processing Systems Conference)</span>, 2013.<br>
        <a href="papers/ridgeNIPS2013.pdf">[PDF]</a>
        <a href="papers/ridge2013appendix.pdf">[Supplementary Information]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">A Risk Comparison of Ordinary Least Squares vs Ridge Regression.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Dean Foster, Sham Kakade, and Lyle Ungar.<br>
        <span class="c-crimson">JMLR</span> <span class="c-yellow">(Journal of Machine Learning Research)</span>, 2013.<br>
        <a href="papers/dhillon13a.pdf">[PDF]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Two Step CCA: A new spectral method for estimating vector models of words.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Jordan Rodu, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">ICML</span><span class="c-yellow">(International Conference on Machine Learning)</span>, 2012.<br>
        <a href="papers/dhillon_icml12_cca.pdf">[PDF]</a> <a href="papers/supplemental_icml12.pdf">[Supplementary Information]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Multi-View Learning of Word Embeddings via CCA.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">NeurIPS</span><span class="c-yellow">(Advances in Neural Information Processing Systems Conference)</span>, 2011.<br>
        <a href="papers/nips11dhillon.pdf">[PDF]</a> <a href="papers/nips11dhillon_supp.pdf">[Supplementary Information]</a>
      </p>
    </li>

    <li>
      <p>
        <em><span class="c-orange">Minimum Description Length Penalization for Group and Multi-Task Sparse Learning.</span></em><br>
        <span class="c-cyan">Paramveer Dhillon</span>, Dean Foster, and Lyle Ungar.<br>
        <span class="c-crimson">JMLR</span> <span class="c-yellow">(Journal of Machine Learning Research)</span>, February 2011.<br>
        <a href="papers/dhillon11a.pdf">[PDF]</a>
      </p>
    </li>
  </ul>

  <!-- Archived / commented-out "Publications (Full List)" block removed to reduce source bloat (it did not affect rendering). -->

  <a name="software" id="software"></a>
  <hr>

  <strong><span class="c-magenta"><u>Software</u></span></strong>

  <p>
    <ol>
      <li>Code and data for our <span class="c-cyan">Nature Human Behaviour 2018</span> paper is available <a href="https://www.dropbox.com/s/iimtqswiesl4skd/inf-max-data-code-release.zip?dl=0">here</a>.</li>
      <li>The ANTsR toolkit for medical image analysis (including the implementation of our <span class="c-cyan">NeuroImage 2014</span> paper) is available <a href="http://stnava.github.io/ANTsR/">here</a>.</li>
      <li>The SWELL (Spectral Word Embedding Learning for Language) JAVA toolkit for inducing word embeddings (cf. <span class="c-cyan">JMLR 2015, ICML 2012, NeurIPS 2011</span>) is available <a href="https://github.com/paramveerdhillon/swell">here</a>.</li>

      <li>
        Various Eigenword (SWELL) embeddings for reproducing the results in our <span class="c-cyan">JMLR 2015</span> paper can be found below <span class="c-cyan">[No additional scaling required for embeddings. Use them as is]</span>. <span class="c-red">[Based on our results, OSCCA and TSCCA embeddings are the most robust and work best on a variety of tasks.]</span>
        <ul style="list-style-type:circle;">
          <li><a href="https://www.dropbox.com/s/ylail17k8yfjhrp/rcv1.oscca.100k.200.c2.gz">OSCCA (h=2)</a> <span class="c-cyan">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=2]</span></li>
          <li><a href="https://www.dropbox.com/s/hb2sscve02k60uf/rcv1.tscca.100k.200.c2.gz">TSCCA (h=2)</a> <span class="c-orange">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=2]</span></li>
          <li><a href="https://www.dropbox.com/s/xcrm1ze5u2yfshf/rcv1.lrmvl1.100k.200.c2.gz">LR-MVL(I) (h=2)</a> <span class="c-cyan">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=2]</span></li>
          <li><a href="https://www.dropbox.com/s/q2xsxxbeay2k3cw/rcv1.lrmvl2.100k.200.gz">LR-MVL(II)</a> <span class="c-orange">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, smooths=0.5</span></li>
          <li><a href="https://www.dropbox.com/s/625z74wwrp1vkdu/rcv1.oscca.100k.200.c10.gz">OSCCA (h=10)</a> <span class="c-cyan">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=10]</span></li>
          <li><a href="https://www.dropbox.com/s/8lhdrrj9ijqxvjc/rcv1.tscca.100k.200.c10.gz">TSCCA (h=10)</a> <span class="c-orange">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=10]</span></li>
          <li><a href="https://www.dropbox.com/s/7xhuk9cf7kk46t7/rcv1.lrmvl1.100k.200.c10.gz">LR-MVL(I) (h=10)</a> <span class="c-cyan">[Trained on Reuters RCV1 (No lowercasing or cleaning). v=100k, k=200, context size (h)=10]</span></li>
        </ul>
      </li>

      <li>
        Generic eigenwords embeddings for various languages <span class="c-red">[Trained on much larger corpora.]</span>
        <ul style="list-style-type:circle;">
          <li><a href="https://www.dropbox.com/s/69ajc3gbabm551r/eigenwords.300k.200.en.zip">English</a> <span class="c-cyan">[Trained on English Gigaword (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/lesbwced7riwdbz/eigenwords.300k.200.de.zip">German</a> <span class="c-orange">[Trained on German Newswire (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/fsii7xdl4yvugpj/eigenwords.300k.200.fr.zip">French</a> <span class="c-cyan">[Trained on French Gigaword (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/f6csj2br51bg0vn/eigenwords.300k.200.es.zip">Spanish</a> <span class="c-orange">[Trained on Spanish Gigaword (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/ie5k09u1rsg4q6e/eigenwords.300k.200.it.zip">Italian</a> <span class="c-cyan">[Trained on Italian Newswire+Wiki  (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/472ftytnbedzncx/eigenwords.300k.200.nl.zip">Dutch</a> <span class="c-orange">[Trained on Dutch Newswire+Wiki (No lowercasing or cleaning). v=300k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/1sdu27pyvd2qx0j/eigenwords.11k.200.charactertokenized.zh.zip">Chinese [Simplified] [Characters]</a> <span class="c-cyan">[Trained on Chinese Gigaword. v=11k, k=200]</span></li>
          <li><a href="https://www.dropbox.com/s/kmj0tdecwmfn336/eigenwords.300k.200.stanfordtokenized.zh.zip">Chinese [Simplified] [Stanford Tokenizer]</a> <span class="c-orange">[Trained on Chinese Gigaword. v=300k, k=200]</span></li>
        </ul>
      </li>
    </ol>
  </p>

  <hr>

  <!-- Default Statcounter code for My University Homepage
  http://pdhillon.com -->
  <script type="text/javascript">
    var sc_project=6254900;
    var sc_invisible=1;
    var sc_security="894aded0";
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter">
      <a title="Web Analytics" href="https://statcounter.com/" target="_blank">
        <img class="statcounter" src="https://c.statcounter.com/6254900/0/894aded0/1/" alt="Web Analytics">
      </a>
    </div>
  </noscript>
  <!-- End of Statcounter Code -->

  <p>
    <span class="c-cyan">Last Modified:</span> 1.7.26
  </p>
</body>
</html>
